{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_with_openai(text, client, prompt):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Chat model\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "                      {\"role\": \"user\", \"content\": text}]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error in generating summary: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "def fetch_death_section_summary(page_title, death_keywords, client):\n",
    "    \"\"\"\n",
    "    Fetches and uses OpenAI to summarize a Wikipedia page section based on death keywords.\n",
    "\n",
    "    Args:\n",
    "    page_title (str): Title of the Wikipedia page.\n",
    "    death_keywords (list): List of keywords to search for in section titles.\n",
    "    openai_api_key (str): OpenAI API key for summary generation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing a brief cause of death and a detailed OpenAI-generated summary.\n",
    "    \"\"\"\n",
    "    url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'parse',\n",
    "        'page': page_title,\n",
    "        'prop': 'text',\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.json()['parse']['text']['*'], 'html.parser')\n",
    "        headings = soup.find_all('span', class_='mw-headline')\n",
    "        death_section_heading = None\n",
    "        for heading in headings:\n",
    "            if any(keyword.lower() in heading.text.lower() for keyword in death_keywords):\n",
    "                death_section_heading = heading\n",
    "                break\n",
    "\n",
    "        if death_section_heading:\n",
    "            content = []\n",
    "            for sibling in death_section_heading.find_parent().find_next_siblings():\n",
    "                if sibling.name in ['h2', 'h3']:\n",
    "                    # Stop if next heading of the same level is found\n",
    "                    break\n",
    "                if sibling.name in ['p', 'ul', 'ol']:\n",
    "                    content.append(sibling.get_text().strip())\n",
    "\n",
    "            full_text = ' '.join(content).strip()\n",
    "            if full_text:\n",
    "                detailed_summary = generate_summary_with_openai(full_text, \n",
    "                                                                client,\n",
    "                                                                \"Summarize the following text:\")\n",
    "                cause_of_death = generate_summary_with_openai(full_text,\n",
    "                                                              client,\n",
    "                                                              \"Return the cause of death in the shortest form possible:\")\n",
    "                return cause_of_death, detailed_summary\n",
    "\n",
    "        return \"No relevant section found\", \"No relevant section found\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: {e}\", f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataframe_with_death_summaries(csv_file_path, death_keywords, client, chunk_size=25, sleep_time=1):\n",
    "    \"\"\"\n",
    "    [Previous Docstring]\n",
    "    \"\"\"\n",
    "    # Load the existing data\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Filter out records that have already been processed (i.e., do not have 'nan' in 'Cause_of_Death')\n",
    "    unprocessed_df = df[df['Cause_of_Death'].isna() & df['Death_Summary'].isna()]\n",
    "\n",
    "    total_rows = len(unprocessed_df)\n",
    "    processed_rows = 0\n",
    "\n",
    "    for index, row in unprocessed_df.iterrows():\n",
    "        try:\n",
    "            # Fetch the death section summary only for unprocessed records\n",
    "            cause, summary = fetch_death_section_summary(row['WIKI_PAGE'], death_keywords, client)\n",
    "            df.at[index, 'Cause_of_Death'] = cause\n",
    "            df.at[index, 'Death_Summary'] = summary\n",
    "        except HTTPError as http_err:\n",
    "            print(f\"HTTP error occurred: {http_err}. Pausing for 30 minutes...\")\n",
    "            time.sleep(1800)  # Wait for 30 minutes\n",
    "            continue\n",
    "\n",
    "        # Increment the processed_rows counter\n",
    "        processed_rows += 1\n",
    "\n",
    "        # Save progress after processing each chunk\n",
    "        if processed_rows % chunk_size == 0 or processed_rows == total_rows:\n",
    "            df.to_csv('wiki_died_output.csv', index=False)\n",
    "            print(f\"Processed {processed_rows}/{total_rows} rows. Sleeping for {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPEN_AI = os.environ.get(\"OPEN_AI\")\n",
    "client = OpenAI(api_key=OPEN_AI)\n",
    "\n",
    "# Define your OpenAI API key and the list of death-related keywords\n",
    "health_keywords = [\"Health\", \"Personal Life\"]\n",
    "death_keywords = [\"Death\", \"Assassination\", \"Demise\", \"Murder\", \"Passed Away\"]\n",
    "\n",
    "# Assuming you have a CSV file named 'wiki_pages.csv' with a column 'WIKI_PAGE'\n",
    "# Replace '/path/to/your/wiki_pages.csv' with the actual path to your CSV file\n",
    "updated_df = augment_dataframe_with_death_summaries('wiki_died_input.csv', death_keywords, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
